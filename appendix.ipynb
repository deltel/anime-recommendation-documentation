{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb32193",
   "metadata": {},
   "source": [
    "## Supplementary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anime-narrowing/anime-reduced.py\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from exceptions.no_data_exception import NoDataException\n",
    "\n",
    "animeDf = pd.read_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\anime_list_11_Jun.csv\")\n",
    "\n",
    "keys = ['mal_id', 'url', 'title', 'type', 'source', 'episodes', 'status', 'premiered', 'duration', 'rating', 'synopsis', 'broadcast', 'producers', 'licensors', 'studios', 'genres', 'themes', 'demographics']\n",
    "\n",
    "def get_anime(id):\n",
    "    url = \"https://api.jikan.moe/v4/anime/{}\".format(id)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise NoDataException(id, response.status_code)\n",
    "    \n",
    "    responseJson = response.json()\n",
    "    return responseJson\n",
    "\n",
    "def get_names(lst):\n",
    "    return ', '.join([item[\"name\"] for item in lst])\n",
    "\n",
    "def extract_data(animeData, index):\n",
    "    try:\n",
    "        data = animeData[\"data\"]\n",
    "    except KeyError:\n",
    "        id = animeData[\"error\"].split('/')[-2]\n",
    "        raise NoDataException(id, animeData[\"status\"])\n",
    "        \n",
    "    malId = data[\"mal_id\"]\n",
    "    url = data[\"url\"]\n",
    "    title = data[\"title\"]\n",
    "    work_type = data[\"type\"]\n",
    "    source = data[\"source\"]\n",
    "    episodes = data[\"episodes\"]\n",
    "    status = data[\"status\"]\n",
    "    premiered = data[\"aired\"][\"from\"]\n",
    "    duration = data[\"duration\"]\n",
    "    rating = data[\"rating\"]\n",
    "    synopsis = data[\"synopsis\"]\n",
    "    broadcast = data[\"broadcast\"][\"string\"]\n",
    "    producers = get_names(data[\"producers\"])\n",
    "    licensors = get_names(data[\"licensors\"])\n",
    "    studios = get_names(data[\"studios\"])\n",
    "    genres = get_names(data[\"genres\"])\n",
    "    themes = get_names(data[\"themes\"])\n",
    "    demographics = get_names(data[\"demographics\"])\n",
    "    \n",
    "    keys = ['mal_id', 'url', 'title', 'type', 'source', 'episodes', 'status', 'premiered', 'duration', 'rating', 'synopsis', 'broadcast', 'producers', 'licensors', 'studios', 'genres', 'themes', 'demographics']\n",
    "    values = [malId, url, title, work_type, source, episodes, status, premiered, duration, rating, synopsis, broadcast, producers, licensors, studios, genres, themes, demographics]\n",
    "    tmpDict = {key: value for key, value in zip(keys, values)}\n",
    "    \n",
    "    return pd.DataFrame(data=tmpDict, index=[index])\n",
    "    \n",
    "\n",
    "failedIds = []\n",
    "index = 1\n",
    "newAnimeDf = pd.read_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\anime_list_12_Jun.csv\")\n",
    "starttime = datetime.now()\n",
    "for id in animeDf.loc[:, \"anime_id\"]:\n",
    "    if index % 3 == 0:\n",
    "        print(\"sleeping for 0.5 seconds\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    if index % 60 == 0:\n",
    "        endtime = datetime.now()\n",
    "        timeelapsed = endtime - starttime\n",
    "        if timeelapsed.microseconds / 1000000 > 59:\n",
    "            print(\"sleeping for 2 seconds\")\n",
    "            time.sleep(2)\n",
    "        starttime = datetime.now()\n",
    "        \n",
    "    print(f\"processing - {id}\")\n",
    "    \n",
    "    try:\n",
    "        animeData = get_anime(id)\n",
    "        df = extract_data(animeData, index)\n",
    "        newAnimeDf = pd.concat([newAnimeDf, df])\n",
    "    except NoDataException as e:\n",
    "        print(f\"failed for id: {id}\")\n",
    "        failedIds += [(e.id, e.status_code)]\n",
    "    \n",
    "    if index % 50 == 0:\n",
    "        newAnimeDf.to_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\anime_list_12_Jun.csv\", index=False)\n",
    "        print(\"saving to file\")\n",
    "    index += 1\n",
    "\n",
    "failedDf = pd.DataFrame(failedIds, columns=[\"anime_id\", \"status_code\"])\n",
    "failedDf.to_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\missing_anime_12_Jun.csv\", index=False)\n",
    "newAnimeDf.to_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\anime_list_12_Jun.csv\", index=False)\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anime-narrowing/reviews.py\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from exceptions.no_data_exception import NoDataException\n",
    "\n",
    "def get_anime(id, page):\n",
    "    url = f\"https://api.jikan.moe/v4/anime/{id}/reviews?page={page}&preliminary=true\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise NoDataException(id, response.status_code)\n",
    "    \n",
    "    responseJson = response.json()\n",
    "    return responseJson\n",
    "\n",
    "def extract_review(data, id):\n",
    "    malId = data[\"mal_id\"]\n",
    "    work_type = data[\"type\"]\n",
    "    date = data[\"date\"]\n",
    "    review = data[\"review\"]\n",
    "    \n",
    "    keys = ('anime_id', 'mal_id', 'type', 'date', 'review')\n",
    "    values = (id, malId, work_type, date, review)\n",
    "    tmpDict = {key: value for key, value in zip(keys, values)}\n",
    "    return pd.DataFrame(data=tmpDict, index=[index])\n",
    "    \n",
    "def extract_data(animeData, id):\n",
    "    try:\n",
    "        data = animeData[\"data\"]\n",
    "    except KeyError:\n",
    "        raise NoDataException(id, animeData[\"status\"])\n",
    "    \n",
    "    df = pd.DataFrame(columns=['anime_id', 'mal_id', 'type', 'date', 'review'])\n",
    "    for review in data:\n",
    "        dfReview = extract_review(review, id)\n",
    "        df = pd.concat([df, dfReview])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def has_more_pages(animeData, id):\n",
    "    return animeData[\"pagination\"][\"has_next_page\"]\n",
    "\n",
    "def sleep_if_necessary(index, starttime):\n",
    "    if index % 3 == 0:\n",
    "        print(\"sleeping for 0.5 seconds\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    if index % 60 == 0:\n",
    "        endtime = datetime.now()\n",
    "        timeelapsed = endtime - starttime\n",
    "        if timeelapsed.microseconds / 1000000 > 59:\n",
    "            print(\"sleeping for 2 seconds\")\n",
    "            time.sleep(2)\n",
    "        starttime = datetime.now()\n",
    "\n",
    "failedIds = []\n",
    "index = 1\n",
    "\n",
    "filelocation = \"E:\\\\applied data science capstone\\\\data\\\\combined\\\\anime_reviews_11_Jun.csv\"\n",
    "reviewsDf = pd.read_csv(filelocation)\n",
    "starttime = datetime.now()\n",
    "\n",
    "animeDf = pd.read_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\anime_list_11_Jun.csv\")\n",
    "animeIds = animeDf[\"anime_id\"]\n",
    "for id in animeIds:\n",
    "    sleep_if_necessary(index, starttime)\n",
    "    \n",
    "    reuse_id = True\n",
    "    j = 1\n",
    "    while reuse_id:\n",
    "        sleep_if_necessary(index, starttime)\n",
    "        print(f\"processing - {id}\")\n",
    "        \n",
    "        try:\n",
    "            animeData = get_anime(id, j)\n",
    "            df = extract_data(animeData, id)\n",
    "            reviewsDf = pd.concat([reviewsDf, df])\n",
    "        except NoDataException as e:\n",
    "            print(f\"failed for id: {id}\")\n",
    "            failedIds += [(e.id, e.status_code)]\n",
    "            \n",
    "        reuse_id = has_more_pages(animeData, id)\n",
    "        index += 1\n",
    "        j += 1\n",
    "        \n",
    "    \n",
    "    if index % 50 == 0:\n",
    "        reviewsDf.to_csv(filelocation, index=False)\n",
    "        print(\"saving to file\")\n",
    "        \n",
    "    index += 1\n",
    "\n",
    "failedDf = pd.DataFrame(failedIds, columns=[\"anime_id\", \"status_code\"])\n",
    "failedDf.to_csv(\"E:\\\\applied data science capstone\\\\data\\\\combined\\\\missing_reviews_11_Jun.csv\", index=False)\n",
    "reviewsDf.to_csv(filelocation, index=False)\n",
    "print(\"finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
